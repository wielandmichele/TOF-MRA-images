{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import random\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import torchio as tio\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from HD5File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_3D_H5 = 'Images/tof_data.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(PATH_3D_H5, 'r') as dd:\n",
    "    print(list(dd.keys()))\n",
    "    print(dd['X'].shape)\n",
    "    print(dd['stroke'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_data(string):\n",
    "    decoded_string = [n.decode(\"UTF-8\", \"ignore\") for n in string]\n",
    "    return(decoded_string)\n",
    "\n",
    "with h5py.File(PATH_3D_H5, 'r') as h5:\n",
    "    print('H5-file: ', list(h5.keys()))\n",
    "    \n",
    "    # Image matrices\n",
    "    X = h5[\"X\"][:]\n",
    "    # Patient ID's\n",
    "    pat = h5[\"pat\"][:]\n",
    "    # Path to images\n",
    "    path = decode_data(h5[\"path\"][:])\n",
    "    # Patient labels (1=stroke, 0=TIA)\n",
    "    Y_pat = h5[\"stroke\"][:]\n",
    "    \n",
    "print(len(X), len(Y_pat), len(pat), len(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation, test split (vgl. NB from Lisa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 508 patients with TOF-MRA images. For training, validation and testing, we need 3 sets.\n",
    "- Training set: 304 images (~60%) -> 211 stroke, 93 non-stroke\n",
    "- Validation set: 102 images (~20%) -> 70 stroke, 32 non-stroke\n",
    "- Test set: 102 images (~20%) -> 70 stroke, 32 non-stroke\n",
    "\n",
    "In every set there is the same percentage of stroke patients vs. non stroke patients (approx. 69% vs. 31%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider stroke and no-stroke patients separately:\n",
    "idx = np.where(Y_pat == 1)[0]\n",
    "stroke_patients = np.unique(pat[idx])\n",
    "idx = np.where(Y_pat == 0)[0]\n",
    "non_stroke_patients = np.unique(pat[idx])\n",
    "print(len(stroke_patients), len(non_stroke_patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle the stroke and non-stroke patients\n",
    "np.random.seed(1)\n",
    "stroke_patients_test = np.random.choice(stroke_patients, size=len(stroke_patients), replace=False)\n",
    "non_stroke_patients_test = np.random.choice(non_stroke_patients, size=len(non_stroke_patients), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set\n",
    "np.random.seed(1)\n",
    "test_tmp = np.concatenate([stroke_patients_test[:70], non_stroke_patients_test[:32]], axis=0)\n",
    "test = np.random.choice(test_tmp, size=len(test_tmp), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_patients_run = [i for i in stroke_patients if i not in test]\n",
    "non_stroke_patients_run = [i for i in non_stroke_patients if i not in test]\n",
    "\n",
    "# randomply shuffle the data\n",
    "np.random.seed(100)\n",
    "stroke_patients_tmp = np.random.choice(stroke_patients_run, size=len(stroke_patients_run), replace=False)\n",
    "non_stroke_patients_tmp = np.random.choice(non_stroke_patients_run, size=len(non_stroke_patients_run), replace=False)\n",
    "print(len(stroke_patients_tmp), len(non_stroke_patients_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = np.concatenate([stroke_patients_tmp[0:211],non_stroke_patients_tmp[:93]], axis=0)\n",
    "valid_tmp = np.concatenate([stroke_patients_tmp[211:len(stroke_patients_tmp)], non_stroke_patients_tmp[93:len(non_stroke_patients)]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle the datasets such that stroke and no-stroke patients are mixed\n",
    "np.random.seed(100)\n",
    "train = np.random.choice(train_tmp, size=len(train_tmp), replace=False)\n",
    "valid = np.random.choice(valid_tmp, size=len(valid_tmp), replace=False)\n",
    "test = np.random.choice(test, size=len(test), replace=False)\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(set_i, X, Y_pat, pat, path):\n",
    "    Y_pat_set = []\n",
    "    pat_set = []\n",
    "    path_set = []\n",
    "    # Find the indices corresponding to the patient_i in set_i\n",
    "    idx = [i for i, pat_i in enumerate(pat) if pat_i in set_i]\n",
    "    X_set = X[idx,:,:,:]\n",
    "    for i in idx:\n",
    "        Y_pat_set.append(Y_pat[i])\n",
    "        pat_set.append(pat[i])\n",
    "        path_set.append(path[i])     \n",
    "    return(X_set, np.array(Y_pat_set), np.array(pat_set), np.array(path_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, pat_train, path_train = get_datasets(train, X, Y_pat, pat, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, Y_valid, pat_valid, path_valid = get_datasets(valid, X, Y_pat, pat, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test, pat_test, path_test = get_datasets(test, X, Y_pat, pat, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dimension to arrays for 3D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train[:,:,:,:,np.newaxis] \n",
    "#X_valid = X_valid[:,:,:,:,np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 50\n",
    "    desired_width = 160\n",
    "    desired_height = 140\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([resize_volume(img) for img in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.array([resize_volume(img) for img in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rotate volumes by random angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train[:10,:,:,:,np.newaxis] \n",
    "#X = X_train[0,:,:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(X[:,:,10]), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = RandomElasticDeformation(num_control_points=4,  locked_borders=1 , max_displacement =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(transformed[:,:,20]), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_dict = {\n",
    "    tio.RandomAffine(): 0.75,\n",
    "    tio.RandomElasticDeformation(): 0.25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tio.RandomElasticDeformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tio.OneOf(transforms_dict)\n",
    "transformed = transform(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = X.shape[0]\n",
    "fig = plt.figure(figsize = (10, 5)) # total figure size (including all subplots)\n",
    "columns = 5\n",
    "rows = 2\n",
    "fig_all = []\n",
    "for i in range(1, X.shape[0]):\n",
    "    img = transform(X[i])\n",
    "    fig_all.append(fig.add_subplot(rows, columns, i))\n",
    "    plt.imshow(np.squeeze(img[:,:,0]), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(transformed[:,:,10]), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "    \n",
    "    volume_shape = volume.shape\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], np.float64)\n",
    "    augmented_volume = tf.reshape(augmented_volume, volume_shape)\n",
    "    return augmented_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocessing(volume, label):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "# Augment the on the fly during training.\n",
    "train_dataset = (\n",
    "    train_loader.map(train_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset.take(10)\n",
    "images, labels = list(data)[0]\n",
    "images = images.numpy()\n",
    "image = images[0]\n",
    "print(\"Dimension of the CT scan is:\", image.shape)\n",
    "plt.imshow(np.squeeze(image[:, :, 10]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset.take(10)\n",
    "images, labels = list(data)[0]\n",
    "images = images.numpy()\n",
    "image = images[0]\n",
    "fig, ax = plt.subplots()\n",
    "pos = ax.imshow(np.squeeze(image[:, :, 10]), cmap=\"gray\")\n",
    "cbar = fig.colorbar(pos, ax=ax)\n",
    "cbar.minorticks_on()\n",
    "fig = plt.gcf()\n",
    "plt.show()\n",
    "fig.savefig('Rotatet volume', dpi = 100,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_slices(num_rows, num_columns, width, height, data):\n",
    "    \"\"\"Plot a montage of 20 CT slices\"\"\"\n",
    "    data = np.rot90(np.array(data))\n",
    "    data = np.transpose(data)\n",
    "    data = np.reshape(data, (num_rows, num_columns, width, height))\n",
    "    rows_data, columns_data = data.shape[0], data.shape[1]\n",
    "    heights = [slc[0].shape[0] for slc in data]\n",
    "    widths = [slc.shape[1] for slc in data[0]]\n",
    "    fig_width = 12.0\n",
    "    fig_height = fig_width * sum(heights) / sum(widths)\n",
    "    f, axarr = plt.subplots(\n",
    "        rows_data,\n",
    "        columns_data,\n",
    "        figsize=(fig_width, fig_height),\n",
    "        gridspec_kw={\"height_ratios\": heights},\n",
    "    )\n",
    "    for i in range(rows_data):\n",
    "        for j in range(columns_data):\n",
    "            axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n",
    "            axarr[i, j].axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    fig.savefig('Rotatet slices', dpi = 100,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize montage of slices.\n",
    "# 4 rows and 10 columns for 100 slices of the CT scan.\n",
    "plot_slices(4, 10, 128, 112, image[:, :, :40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 128\n",
    "h = 112\n",
    "d = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(width=w, height=h, depth=d):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2, padding = 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2, padding = 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2, padding = 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2, padding = 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x) #oder 0.6\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model.\n",
    "model = get_model(width=w, height=h, depth=d)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.96, staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = lr_schedule),\n",
    "    #optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "#model.compile(loss=\"binary_crossentropy\", optimizer = keras.optimizers.RMSprop(lr=1e-4), metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks.\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15),\n",
    "    keras.callbacks.ModelCheckpoint(\"3d_image_classification.h5\", save_best_only=True),\n",
    "    #keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10)   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=1, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, metric in enumerate([\"acc\", \"loss\"]):\n",
    "    ax[i].plot(model.history.history[metric])\n",
    "    ax[i].plot(model.history.history[\"val_\" + metric])\n",
    "    ax[i].set_title(\"Model {}\".format(metric))\n",
    "    ax[i].set_xlabel(\"epochs\")\n",
    "    ax[i].set_ylabel(metric)\n",
    "    ax[i].legend([\"train\", \"val\"])\n",
    "    \n",
    "ax[0].set_ylim(0,1)\n",
    "ax[1].set_ylim(0)\n",
    "\n",
    "p = 'Modell_SGD_BS4'\n",
    "fig.savefig(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.expand_dims(X_valid, axis = 4)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"3d_image_classification.h5\")\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred = (y_pred > 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.confusion_matrix(labels = Y_valid, predictions = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decayed_learning_rate(step, initial_learning_rate, decay_rate, decay_steps):\n",
    "  return initial_learning_rate * decay_rate ** (step / decay_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "steps = 152\n",
    "list = []\n",
    "for i in range(0,epochs):\n",
    "    rate = decayed_learning_rate(steps*i, 0.0001, 0.96, 1000)\n",
    "    list.append(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
