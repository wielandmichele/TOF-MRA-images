{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn\n",
    "import tempfile\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from functions.auc_delong_xu import auc_ci_Delong\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import Session\n",
    "import gc\n",
    "\n",
    "from Functions.data_augmentation import *\n",
    "from Functions.data_import import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-marker",
   "metadata": {},
   "source": [
    "### Import data from HD5File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_3D_H5 = 'Images/preprocessed_data_128_112_40.hdf5'\n",
    "with h5py.File(PATH_3D_H5, 'r') as h5:\n",
    "    print('H5-file: ', list(h5.keys()))\n",
    "\n",
    "    X = h5[\"X\"][:]\n",
    "    Y_pat = h5[\"Y_pat\"][:]\n",
    "    pat = h5[\"pat\"][:]\n",
    "\n",
    "print(X.shape, X.min(), X.max(), X.mean(), X.std(), Y_pat.shape, pat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def augment_3d_two(volume):\n",
    "    \"\"\"Randomly pick two data augmentation functions for every image\"\"\"\n",
    "\n",
    "    def augment(volume):\n",
    "        rand = np.random.randint(0,5, size = 2)\n",
    "            \n",
    "        if 0 in rand:\n",
    "            volume = random_zoom3d(volume, 0.8,1.3) \n",
    "        if 1 in rand:\n",
    "            volume = random_rotate3d(volume, -20, 20, -5, -5, -5, -5)\n",
    "        if 2 in rand:\n",
    "            volume = random_shift3d(volume, -20, 20, -20, 20, 0, 0) #do not shift in z direction\n",
    "        if 3 in rand:\n",
    "            volume = random_flip3d(volume)\n",
    "        if 4 in rand:\n",
    "            volume = random_gaussianfilter3d(volume, 0.2)\n",
    "            \n",
    "        return volume\n",
    "    \n",
    "    volume_shape = volume.shape\n",
    "    augmented_volume = tf.numpy_function(augment, [volume], np.float64)\n",
    "    augmented_volume = tf.reshape(augmented_volume, volume_shape)\n",
    "    return augmented_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocessing(volume, label):\n",
    "    volume = augment_3d_two(volume)\n",
    "    return volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(X_train, Y_train, X_valid, Y_valid):\n",
    "    Y_train = to_categorical(Y_train)\n",
    "    Y_valid = to_categorical(Y_valid)\n",
    "    \n",
    "    bool_train_labels = Y_train[:,1] != 0\n",
    "    pos_features = X_train[bool_train_labels]\n",
    "    neg_features = X_train[~bool_train_labels]\n",
    "    pos_labels = Y_train[bool_train_labels]\n",
    "    neg_labels = Y_train[~bool_train_labels]\n",
    "    \n",
    "    def make_ds(features, labels):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n",
    "        ds = ds.shuffle(len(pos_features)*2).repeat()\n",
    "        return ds\n",
    "\n",
    "    pos_ds = make_ds(pos_features, pos_labels)\n",
    "    neg_ds = make_ds(neg_features, neg_labels)\n",
    "    \n",
    "    resampled_ds = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\n",
    "    validation_loader = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid))\n",
    "\n",
    "    batch_size = 2\n",
    "    # Augment the on the fly during training.\n",
    "    train_dataset = (\n",
    "        resampled_ds.shuffle(buffer_size = (len(pos_features)*2), reshuffle_each_iteration=True)\n",
    "        .map(train_preprocessing)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(2))\n",
    "\n",
    "    validation_dataset = (\n",
    "        validation_loader.shuffle(len(X_valid))\n",
    "        .batch(batch_size)\n",
    "        .prefetch(2))\n",
    "    \n",
    "    pos = len(pos_features)\n",
    "    neg = len(neg_features)\n",
    "    total = pos + neg\n",
    "    resampled_steps_per_epoch = np.ceil(2.0*pos/batch_size)\n",
    "    \n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    \n",
    "    return train_dataset, validation_dataset, class_weight, batch_size, resampled_steps_per_epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(IMAGE_DIMENSION):\n",
    "    inputs = keras.Input(IMAGE_DIMENSION)\n",
    "\n",
    "    x = layers.Conv3D(filters=8, kernel_size=3, activation=\"relu\", padding = 'same')(inputs)\n",
    "    x = layers.AveragePooling3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=16, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
    "    x = layers.AveragePooling3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
    "    x = layers.AveragePooling3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
    "    x = layers.AveragePooling3D(pool_size = 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\", padding = 'same')(x)\n",
    "    x = layers.AveragePooling3D(pool_size = 2, padding = 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    x = layers.Dense(units=64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.01)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=2, activation=\"softmax\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "\n",
    "## get stroke and tia indeces\n",
    "stroke_idx = np.where(Y_pat == 1)\n",
    "tia_idx = np.where(Y_pat == 0)\n",
    "\n",
    "## shuffle indeces\n",
    "np.random.seed(2021)\n",
    "np.random.shuffle(stroke_idx[0])\n",
    "np.random.shuffle(tia_idx[0])\n",
    "\n",
    "## split indeces into 5 parts\n",
    "splits_stroke = np.array_split(stroke_idx[0],N_FOLDS)\n",
    "splits_tia = np.array_split(tia_idx[0], [31,62,93,125])\n",
    "\n",
    "## define chosen splits for each fold\n",
    "test_folds = [0, 1, 2, 3, 4]\n",
    "valid_folds = [1, 2, 3, 4, 0]\n",
    "train_folds = [[0, 1], [1, 2], [2, 3], [3, 4], [0, 4]] ## remove these splits for training data\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    \n",
    "    ## define train, test and validation splits\n",
    "    test_idx = np.concatenate((splits_stroke[test_folds[fold]], splits_tia[test_folds[fold]]), axis = None)\n",
    "    valid_idx = np.concatenate((splits_stroke[valid_folds[fold]], splits_tia[valid_folds[fold]]), axis = None)\n",
    "\n",
    "    train_stroke = np.delete(splits_stroke, train_folds[fold], 0)\n",
    "    train_stroke = [item for sublist in train_stroke for item in sublist]\n",
    "    \n",
    "    train_tia = np.delete(splits_tia, train_folds[fold], 0)\n",
    "    train_tia = [item for sublist in train_tia for item in sublist]\n",
    "    \n",
    "    train_idx = np.concatenate((train_stroke, train_tia), axis = None)\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    X_valid = X[valid_idx]\n",
    "    \n",
    "    Y_train = Y_pat[train_idx]\n",
    "    Y_test = Y_pat[test_idx]\n",
    "    Y_valid = Y_pat[valid_idx]\n",
    "    \n",
    "    pat_train = pat[train_idx]\n",
    "    pat_test = pat[test_idx]\n",
    "    pat_valid = pat[valid_idx]\n",
    "    \n",
    "    #get resampled dataset\n",
    "    train_dataset, validation_dataset, class_weight, batch_size, resampled_steps_per_epoch = get_dataset(X_train, Y_train, X_valid, Y_valid)\n",
    "    \n",
    "    #get model\n",
    "    model = get_model(X_train[0].shape)\n",
    "    print(model.summary())\n",
    "    \n",
    "    ### define metrics\n",
    "    metrics = [\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.CategoricalCrossentropy(name=\"categorical_crossentropy\")]\n",
    "\n",
    "    ### prepare files for logging\n",
    "    results_filepath = 'results'+str(fold)+'.csv'\n",
    "    if os.path.exists(results_filepath):\n",
    "        os.remove(results_filepath)\n",
    "    \n",
    "    history_filepath = 'History'+str(fold)\n",
    "    if os.path.isdir(history_filepath):\n",
    "        shutil.rmtree(history_filepath )\n",
    "    os.makedirs(history_filepath )\n",
    "\n",
    "    epochs_filepath = history_filepath+'/model.epoch{epoch:02d}.hdf5'\n",
    "\n",
    "    ### define callback_list\n",
    "    callback_list = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=epochs_filepath, save_freq='epoch', verbose=1), \n",
    "    keras.callbacks.CSVLogger(results_filepath)]\n",
    "\n",
    "    ### compile model\n",
    "    model.compile(loss = \"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate =  0.00001),\n",
    "    metrics = metrics)\n",
    "    \n",
    "    ###train model\n",
    "    epochs = 150\n",
    "    hist = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        verbose=1, callbacks=callback_list, \n",
    "        steps_per_epoch = resampled_steps_per_epoch,\n",
    "        class_weight = class_weight)\n",
    "    \n",
    "    ###use epoch with minimal validation loss from the tenth epoch\n",
    "    dat = pd.read_csv(results_filepath, index_col='epoch')\n",
    "    best_model = np.where(dat.val_loss == np.min(dat.val_loss[10:]))[0][0] \n",
    "    best_model = best_model + 1\n",
    "    model.load_weights(history_filepath+'/model.epoch'+str(best_model)+'.hdf5')\n",
    "    \n",
    "    y_prob = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = (y_prob[:,1] > 0.5).astype(np.int)\n",
    "    \n",
    "    #calculate categorical crossentropy\n",
    "    Y_test_cat = to_categorical(Y_test)\n",
    "    m = tf.keras.metrics.CategoricalCrossentropy()\n",
    "    m.update_state(Y_test_cat, y_prob)\n",
    "    catcrossentropy = m.result().numpy()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df.loc[:,\"pat_id\"] = list(pat_test)\n",
    "    df.loc[:,\"y_test\"] = Y_test\n",
    "    df.loc[:,\"y_pred\"] = y_pred\n",
    "    df.loc[:,\"y_prob\"] = y_prob[:,1]\n",
    "    df.loc[:,\"cat_cross\"] = list(np.repeat(catcrossentropy, len(y_pred)))\n",
    "    df.loc[:,\"fold\"] = list(np.repeat(fold, len(y_pred)))\n",
    "    \n",
    "    df.to_csv(\"predictions\"+str(fold)+\".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge predictions from 5folds to one file\n",
    "pred0 = pd.read_csv('predictions0.csv', index_col = False)\n",
    "pred1 = pd.read_csv('predictions1.csv', index_col = False)\n",
    "pred2 = pd.read_csv('predictions2.csv', index_col = False)\n",
    "pred3 = pd.read_csv('predictions3.csv', index_col = False)\n",
    "pred4 = pd.read_csv('predictions4.csv', index_col = False)\n",
    "\n",
    "merged = pd.concat([pred0, pred1, pred2, pred3, pred4], axis=0)\n",
    "merged = merged.reset_index()\n",
    "merged.to_csv('pred5fold.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
