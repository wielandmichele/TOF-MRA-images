{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n",
    "!pip install hyperas\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import Session\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    ##Import data from hd5f file\n",
    "    PATH_3D_H5 = 'Images/preprocessed_data_128_112_40_outcome.hdf5'\n",
    "    with h5py.File(PATH_3D_H5, 'r') as h5:\n",
    "        print('H5-file: ', list(h5.keys()))\n",
    "\n",
    "        X = h5[\"X\"][:]\n",
    "        Y_pat = h5[\"Y_outcome\"][:]\n",
    "        pat = h5[\"pat\"][:]\n",
    "    \n",
    "    N_FOLDS = 5\n",
    "\n",
    "    ## get stroke and tia indeces\n",
    "    bad_outcome_idx = np.where(Y_pat == 1)\n",
    "    good_outcome_idx = np.where(Y_pat == 0)\n",
    "\n",
    "    ## shuffle indeces\n",
    "    np.random.seed(2021)\n",
    "    np.random.shuffle(bad_outcome_idx[0])\n",
    "    np.random.shuffle(good_outcome_idx[0])\n",
    "\n",
    "    ## split indeces into 5 parts\n",
    "    splits_bad_outcome = np.array_split(bad_outcome_idx[0],N_FOLDS)\n",
    "    splits_good_outcome = np.array_split(good_outcome_idx[0], N_FOLDS)\n",
    "\n",
    "    ## define chosen splits for each fold\n",
    "    test_folds = [0, 1, 2, 3, 4]\n",
    "    valid_folds = [1, 2, 3, 4, 0]\n",
    "    train_folds = [[0, 1], [1, 2], [2, 3], [3, 4], [0, 4]] ## remove these splits for training data\n",
    "\n",
    "    fold = 4\n",
    "    \n",
    "    ## define train, test and validation splits\n",
    "    test_idx = np.concatenate((splits_bad_outcome[test_folds[fold]], splits_good_outcome[test_folds[fold]]), axis = None)\n",
    "    valid_idx = np.concatenate((splits_bad_outcome[valid_folds[fold]], splits_good_outcome[valid_folds[fold]]), axis = None)\n",
    "\n",
    "    train_bad_outcome = np.delete(splits_bad_outcome, train_folds[fold], 0)\n",
    "    train_bad_outcome = [item for sublist in train_bad_outcome for item in sublist]\n",
    "\n",
    "    train_good_outcome = np.delete(splits_good_outcome, train_folds[fold], 0)\n",
    "    train_good_outcome = [item for sublist in train_good_outcome for item in sublist]\n",
    "\n",
    "    train_idx = np.concatenate((train_good_outcome, train_bad_outcome), axis = None)\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    X_valid = X[valid_idx]\n",
    "\n",
    "    Y_train = Y_pat[train_idx]\n",
    "    Y_test = Y_pat[test_idx]\n",
    "    Y_valid = Y_pat[valid_idx]\n",
    "\n",
    "    pat_train = pat[train_idx]\n",
    "    pat_test = pat[test_idx]\n",
    "    pat_valid = pat[valid_idx]\n",
    "    \n",
    "    ##Labels to categorical\n",
    "    Y_train = to_categorical(Y_train)\n",
    "    Y_valid = to_categorical(Y_valid)\n",
    "    \n",
    "    ###Create balanced dataset\n",
    "    bool_train_labels = Y_train[:,1] != 0\n",
    "    pos_features = X_train[bool_train_labels]\n",
    "    neg_features = X_train[~bool_train_labels]\n",
    "    pos_labels = Y_train[bool_train_labels]\n",
    "    neg_labels = Y_train[~bool_train_labels]\n",
    "\n",
    "    ids = np.arange(len(pos_features))\n",
    "    choices = np.random.choice(ids, len(neg_features))\n",
    "    res_pos_features = pos_features[choices]\n",
    "    res_pos_labels = pos_labels[choices]\n",
    "\n",
    "    resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\n",
    "    resampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n",
    "\n",
    "    order = np.arange(len(resampled_labels))\n",
    "    np.random.shuffle(order)\n",
    "    X_train_balanced = resampled_features[order]\n",
    "    Y_train_balanced = resampled_labels[order]\n",
    "\n",
    "    print(X_train_balanced.shape, Y_train_balanced.shape)\n",
    "    \n",
    "    return X_train_balanced, Y_train_balanced, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_valid, Y_valid):\n",
    "    num_classes = 2\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Convolution3D({{choice([8, 16, 32])}}, kernel_size=3, activation=\"relu\", \n",
    "                                   batch_input_shape=(None,128,112,40,1), padding = 'same'))\n",
    "    model.add(layers.MaxPooling3D(pool_size = 2))\n",
    "    model.add(layers.BatchNormalization(center = True, scale = True))\n",
    "    \n",
    "    model.add(layers.Convolution3D({{choice([16, 32, 64])}}, kernel_size=3, activation=\"relu\", padding = 'same'))\n",
    "    model.add(layers.MaxPooling3D(pool_size = 2))\n",
    "    model.add(layers.BatchNormalization(center = True, scale = True))\n",
    "    \n",
    "    model.add(layers.Convolution3D({{choice([16, 32, 64])}}, kernel_size=3, activation=\"relu\", padding = 'same'))\n",
    "    model.add(layers.MaxPooling3D(pool_size = 2))\n",
    "    model.add(layers.BatchNormalization(center = True, scale = True))\n",
    "    \n",
    "    model.add(layers.Convolution3D({{choice([32, 64, 128])}}, kernel_size=3, activation=\"relu\", padding = 'same'))\n",
    "    model.add(layers.MaxPooling3D(pool_size = 2))\n",
    "    model.add(layers.BatchNormalization(center = True, scale = True))\n",
    "    \n",
    "    model.add(layers.Convolution3D({{choice([32, 64, 128])}}, kernel_size=3, activation=\"relu\", padding = 'same'))\n",
    "    model.add(layers.MaxPooling3D(pool_size = 2))\n",
    "    model.add(layers.BatchNormalization(center = True, scale = True))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense({{choice([32, 64, 128])}}, activation=\"relu\"))\n",
    "    model.add(layers.Dropout({{uniform(0, 0.6)}}))\n",
    "    \n",
    "    # If we choose 'two', add an additional second dense layer\n",
    "    if {{choice(['one', 'two'])}} == 'two':\n",
    "        model.add(layers.Dense({{choice([32, 64, 128])}}, activation=\"relu\"))\n",
    "        model.add(layers.Dropout({{uniform(0, 0.6)}}))\n",
    "    \n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "              \n",
    "    #compile model\n",
    "    metrics = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.AUC(name='auc')]\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate =  0.00001),\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    result = model.fit(X_train_balanced, Y_train_balanced,\n",
    "                        validation_data=(X_valid, Y_valid), \n",
    "                        batch_size=4,\n",
    "                        verbose=2,\n",
    "                        epochs=50)\n",
    "    \n",
    "    #get the highest validation loss of the training epochs\n",
    "    val_loss = np.amin(result.history['val_loss']) \n",
    "    print('Best validation loss of epoch:', val_loss)\n",
    "              \n",
    "    return {'loss': -val_loss, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      max_evals=50,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      notebook_name=\"Modell_Outcome_Prognose_Hyperparameter_Optimierung\", # Without this it can't find the notebook!\n",
    "                                      trials=Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
